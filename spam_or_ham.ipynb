{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam or Ham: Basic Naive Bayes Email Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My aim in this project is use Bayes formula to construct a spam filter, which will predict based off of raw text whether a message is \"spam\" or \"ham (non-spam)\". The problem is one of binary classification in which my model will be appraised as accurate if it properly predicts the right label. The main accuracy metric, therefore, will be the number of correct labels divided by the number of overall guesses.  \n",
    "\n",
    "The dataset for this project is from the Data Science/Machine Learning competition website <a href=\"https://www.kaggle.com/uciml/sms-spam-collection-dataset\" target=\"#\">Kaggle</a>. The dataset is comprised of two columns, one for the message text and one for the label (\"ham\" or \"spam\"). There are 5169 rows of unique data."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "What is the probability (Pr) that a message is spam (S) given word (W)? This question is what <a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering\" target=\"#\">Bayes' theorem</a> attempts to calculate. Divide the probability of a word being in a spam message by that same probability added to the probability of a word being in a ham message -- this is a simplified form of Bayes' theorem that excludes any prior assumptions regarding the probability of a message being ham or spam. To be more specific, in all of the spam messages, count the number of times a given word occurs and divide that by the number of overall words in the spam messages -- this constitutes the probability in the numerator. This probability is then added to the same calculation applied to the word's frequency in ham messages -- the denominator. \n",
    "\n",
    "Since each message contains multiple words, probabilities for each word given spam are multiplied, then the resulting product is divided by the same product added to the product of all the multiplied words given ham. The \"naive\" in Naive Bayes owes to the fact that multiplying probabilities together assumes a statistical independence these probabilities don't have -- and why's that? Because words aren't randomly assembled into sentences. Certain words occur frequently with others, but for the sake of simplicity, we're going to ignore this fact. Ignoring the patterns in language isn't foolhardy or \"naive\" in the pejorative sense, as model builders much more sophisticated than myself have proven time and again that Naive Bayes works really well for problems like spam filtering.\n",
    "\n",
    "While there are prebuilt Bayesian models provided by many popular Machine Learning libraries, for the sake of learning, I'll be building my own from scratch, only using libraries to preprocess and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read CSV file into a dataframe\n",
    "data = pd.read_csv(\"spam.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine top 5 rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the additional columns (Unnamed:2 through Unnamed: 4) have some text data,\n",
    "#but are primarily composed of null values and will be dropped.\n",
    "data = data[['v1','v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns for easier reference\n",
    "data.columns = ['label','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11629 in the dataframe after removing non-alphanumeric characters.\n"
     ]
    }
   ],
   "source": [
    "#create list of all the unique words that occur in the text col\n",
    "unique_words = []\n",
    "\n",
    "for i,row in data.iterrows():\n",
    "    for word in row['text'].split(' '):\n",
    "        word = re.sub(r'\\W+', '', word)\n",
    "        if word in unique_words:\n",
    "            pass\n",
    "        else:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "total_num_of_words = len(unique_words)\n",
    "\n",
    "print(\"There are {} in the dataframe after removing non-alphanumeric characters.\".format(total_num_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Go</th>\n",
       "      <th>until</th>\n",
       "      <th>jurong</th>\n",
       "      <th>point</th>\n",
       "      <th>crazy</th>\n",
       "      <th>Available</th>\n",
       "      <th>only</th>\n",
       "      <th>in</th>\n",
       "      <th>bugis</th>\n",
       "      <th>n</th>\n",
       "      <th>...</th>\n",
       "      <th>BORING</th>\n",
       "      <th>salesman</th>\n",
       "      <th>REMINDER</th>\n",
       "      <th>å750</th>\n",
       "      <th>087187272008</th>\n",
       "      <th>NOW1</th>\n",
       "      <th>Pity</th>\n",
       "      <th>Soany</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>bitching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11629 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Go  until  jurong  point  crazy  Available  only  in  bugis  n  ...  \\\n",
       "0   1      1       1      1      1          1     1   1      1  1  ...   \n",
       "1   0      0       0      0      0          0     0   0      0  0  ...   \n",
       "2   0      0       0      0      0          0     0   1      0  0  ...   \n",
       "3   0      0       0      0      0          0     0   0      0  0  ...   \n",
       "4   0      0       0      0      0          0     0   0      0  0  ...   \n",
       "\n",
       "   BORING  salesman  REMINDER  å750  087187272008  NOW1  Pity  Soany  \\\n",
       "0       0         0         0     0             0     0     0      0   \n",
       "1       0         0         0     0             0     0     0      0   \n",
       "2       0         0         0     0             0     0     0      0   \n",
       "3       0         0         0     0             0     0     0      0   \n",
       "4       0         0         0     0             0     0     0      0   \n",
       "\n",
       "   suggestions  bitching  \n",
       "0            0         0  \n",
       "1            0         0  \n",
       "2            0         0  \n",
       "3            0         0  \n",
       "4            0         0  \n",
       "\n",
       "[5 rows x 11629 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create empty dataframe to store word counts\n",
    "wordcount_df = pd.DataFrame(0, index=np.arange(len(data)), columns=unique_words)\n",
    "\n",
    "#count occurence of words in each message and increment the corresponding column\n",
    "for i, row in data.iterrows():\n",
    "    for word in row['text'].split(' '):\n",
    "        word = re.sub(r'\\W+', '', word)\n",
    "        wordcount_df.iloc[i,unique_words.index(word)]+=1\n",
    "\n",
    "wordcount_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine word count dataframe with labels from original dataframe\n",
    "merged = pd.concat([wordcount_df,data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Go</th>\n",
       "      <th>until</th>\n",
       "      <th>jurong</th>\n",
       "      <th>point</th>\n",
       "      <th>crazy</th>\n",
       "      <th>Available</th>\n",
       "      <th>only</th>\n",
       "      <th>in</th>\n",
       "      <th>bugis</th>\n",
       "      <th>n</th>\n",
       "      <th>...</th>\n",
       "      <th>REMINDER</th>\n",
       "      <th>å750</th>\n",
       "      <th>087187272008</th>\n",
       "      <th>NOW1</th>\n",
       "      <th>Pity</th>\n",
       "      <th>Soany</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>bitching</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11631 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Go  until  jurong  point  crazy  Available  only  in  bugis  n  ...  \\\n",
       "0   1      1       1      1      1          1     1   1      1  1  ...   \n",
       "1   0      0       0      0      0          0     0   0      0  0  ...   \n",
       "2   0      0       0      0      0          0     0   1      0  0  ...   \n",
       "3   0      0       0      0      0          0     0   0      0  0  ...   \n",
       "4   0      0       0      0      0          0     0   0      0  0  ...   \n",
       "\n",
       "   REMINDER  å750  087187272008  NOW1  Pity  Soany  suggestions  bitching  \\\n",
       "0         0     0             0     0     0      0            0         0   \n",
       "1         0     0             0     0     0      0            0         0   \n",
       "2         0     0             0     0     0      0            0         0   \n",
       "3         0     0             0     0     0      0            0         0   \n",
       "4         0     0             0     0     0      0            0         0   \n",
       "\n",
       "   label                                               text  \n",
       "0    ham  Go until jurong point, crazy.. Available only ...  \n",
       "1    ham                      Ok lar... Joking wif u oni...  \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "3    ham  U dun say so early hor... U c already then say...  \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...  \n",
       "\n",
       "[5 rows x 11631 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine first five rows of merged dataframe\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Class object to read in training dataset, calculate conditional probabilities\n",
    "#and create model to generate predictions for test dataset\n",
    "class PredictBayes:\n",
    "    def __init__(self):\n",
    "        self.priors = {}\n",
    "        self.conditionals = {}\n",
    "        self.predictions = []\n",
    "        self.prob_product = {}\n",
    "        \n",
    "    def fit(self,train,target):\n",
    "        self.target = target\n",
    "        self.train = train\n",
    "        self.labels = set(self.train[self.target].values)\n",
    "\n",
    "        for label in self.labels:\n",
    "            self.priors[label] = self.train[self.train[target]==label].shape[0]/self.train.shape[0]\n",
    "            data = self.train[self.train[self.target]==label]\n",
    "            self.conditionals[label] = data.drop(self.target,axis=1).sum()/data.drop(self.target,axis=1).sum().sum()\n",
    "            \n",
    "    def predict(self,test,features):\n",
    "        self.test = test\n",
    "        self.features = features\n",
    "        for label in self.labels:\n",
    "            product_list = []\n",
    "            for word in self.features.split(' '):\n",
    "                word = re.sub(r'\\W+', '', word)\n",
    "                product_list.append(self.conditionals[label][word])\n",
    "            self.prob_product[label] = product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = PredictBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test sets -- 80% and 20% of the data, respetively\n",
    "train, test = train_test_split(merged,test_size=.2,random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.fit(train.drop('text',axis=1),\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
